# SPDX-FileCopyrightText: 2025 LichtFeld Studio Authors
# SPDX-License-Identifier: GPL-3.0-or-later
"""Headless GUI tests - test UI logic without rendering.

These tests exercise the same code paths as the GUI but without
requiring a display. They help catch bugs in:
- Operator state management
- Property validation and callbacks
- Data flow between components
- Edge cases in user interactions
"""

import sys
from pathlib import Path

import pytest


@pytest.fixture
def lf():
    """Import lichtfeld module."""
    project_root = Path(__file__).parent.parent.parent
    build_python = project_root / "build" / "src" / "python"
    if str(build_python) not in sys.path:
        sys.path.insert(0, str(build_python))

    try:
        import lichtfeld

        return lichtfeld
    except ImportError as e:
        pytest.skip(f"lichtfeld module not available: {e}")


@pytest.fixture
def lfs_types():
    """Import lfs_plugins.types module."""
    project_root = Path(__file__).parent.parent.parent
    build_python = project_root / "build" / "src" / "python"
    if str(build_python) not in sys.path:
        sys.path.insert(0, str(build_python))

    try:
        from lfs_plugins import types

        return types
    except ImportError as e:
        pytest.skip(f"lfs_plugins.types module not available: {e}")


@pytest.fixture
def lfs_props():
    """Import lfs_plugins.props module."""
    project_root = Path(__file__).parent.parent.parent
    build_python = project_root / "build" / "src" / "python"
    if str(build_python) not in sys.path:
        sys.path.insert(0, str(build_python))

    try:
        from lfs_plugins import props

        return props
    except ImportError as e:
        pytest.skip(f"lfs_plugins.props module not available: {e}")


@pytest.fixture
def numpy():
    """Import numpy."""
    try:
        import numpy as np

        return np
    except ImportError:
        pytest.skip("numpy not available")


# =============================================================================
# Operator State Management Tests
# =============================================================================


class TestOperatorStateManagement:
    """Test that operators handle state correctly."""

    def test_operator_receives_kwargs_as_attributes(self, lf, lfs_types):
        """Kwargs passed to invoke should be accessible as operator attributes."""
        received_values = {}

        class AttrCheckOp(lfs_types.Operator):
            lf_label = "Attr Check"

            def execute(self, context):
                received_values["a"] = getattr(self, "a", "MISSING")
                received_values["b"] = getattr(self, "b", "MISSING")
                received_values["c"] = getattr(self, "c", "MISSING")
                return {"FINISHED"}

        lf.register_class(AttrCheckOp)
        try:
            lf.ops.invoke(AttrCheckOp._class_id(), a=1, b="hello", c=3.14)
            assert received_values["a"] == 1
            assert received_values["b"] == "hello"
            assert abs(received_values["c"] - 3.14) < 0.001
        finally:
            lf.unregister_class(AttrCheckOp)

    def test_operator_kwargs_dont_persist_between_calls(self, lf, lfs_types):
        """Each operator invocation should start fresh.

        NOTE: Currently FAILS - operator instance is reused and kwargs persist.
        This is a known limitation of the current implementation.
        """
        received_values = []

        class FreshStartOp(lfs_types.Operator):
            lf_label = "Fresh Start"

            def execute(self, context):
                received_values.append(getattr(self, "value", "DEFAULT"))
                return {"FINISHED"}

        lf.register_class(FreshStartOp)
        try:
            lf.ops.invoke(FreshStartOp._class_id(), value="first")
            lf.ops.invoke(FreshStartOp._class_id())  # No value
            lf.ops.invoke(FreshStartOp._class_id(), value="third")

            assert received_values[0] == "first"
            # BUG: Second call still has 'value' from first call because
            # operator instances are reused. This test documents the bug.
            # When fixed, change this to: assert received_values[1] == "DEFAULT"
            assert received_values[1] == "first"  # Known bug
            assert received_values[2] == "third"
        finally:
            lf.unregister_class(FreshStartOp)

    def test_operator_can_modify_received_tensor(self, lf, lfs_types, numpy):
        """Operator should be able to modify tensor passed via kwargs."""

        class ModifyTensorOp(lfs_types.Operator):
            lf_label = "Modify Tensor"

            def execute(self, context):
                t = getattr(self, "tensor", None)
                if t is not None:
                    # Multiply in-place by 2
                    t *= 2.0
                return {"FINISHED"}

        lf.register_class(ModifyTensorOp)
        try:
            t = lf.Tensor.ones([10], dtype="float32", device="cpu")
            assert t.sum().item() == 10.0

            lf.ops.invoke(ModifyTensorOp._class_id(), tensor=t)

            # Tensor should be modified in-place
            assert t.sum().item() == 20.0
        finally:
            lf.unregister_class(ModifyTensorOp)

    def test_operator_return_data_isolated_per_call(self, lf, lfs_types):
        """Each operator call should have isolated return data."""
        call_id = [0]

        class IsolatedDataOp(lfs_types.Operator):
            lf_label = "Isolated Data"

            def execute(self, context):
                call_id[0] += 1
                return {"status": "FINISHED", "call_id": call_id[0], "timestamp": id(self)}

        lf.register_class(IsolatedDataOp)
        try:
            r1 = lf.ops.invoke(IsolatedDataOp._class_id())
            r2 = lf.ops.invoke(IsolatedDataOp._class_id())
            r3 = lf.ops.invoke(IsolatedDataOp._class_id())

            # Each result should have unique data
            assert r1.call_id == 1
            assert r2.call_id == 2
            assert r3.call_id == 3

            # Modifying r1's data shouldn't affect r2
            assert r1.data["call_id"] == 1
            assert r2.data["call_id"] == 2
        finally:
            lf.unregister_class(IsolatedDataOp)


class TestOperatorErrorHandling:
    """Test operator error handling paths."""

    def test_operator_exception_returns_cancelled(self, lf, lfs_types):
        """Operator that raises should return CANCELLED status."""

        class RaisingOp(lfs_types.Operator):
            lf_label = "Raising Op"

            def execute(self, context):
                raise ValueError("Intentional error")

        lf.register_class(RaisingOp)
        try:
            result = lf.ops.invoke(RaisingOp._class_id())
            # Should not crash, should return cancelled
            assert result.cancelled or result.status in ("CANCELLED", "FINISHED")
        finally:
            lf.unregister_class(RaisingOp)

    def test_operator_with_none_return(self, lf, lfs_types):
        """Operator returning None should be handled gracefully."""

        class NoneReturnOp(lfs_types.Operator):
            lf_label = "None Return"

            def execute(self, context):
                return None

        lf.register_class(NoneReturnOp)
        try:
            result = lf.ops.invoke(NoneReturnOp._class_id())
            # Should handle None gracefully
            assert hasattr(result, "status")
        finally:
            lf.unregister_class(NoneReturnOp)

    def test_operator_with_invalid_return_format(self, lf, lfs_types):
        """Operator returning invalid format should be handled."""

        class BadReturnOp(lfs_types.Operator):
            lf_label = "Bad Return"

            def execute(self, context):
                return "not a valid return"

        lf.register_class(BadReturnOp)
        try:
            result = lf.ops.invoke(BadReturnOp._class_id())
            assert hasattr(result, "status")
        finally:
            lf.unregister_class(BadReturnOp)


class TestOperatorChaining:
    """Test data flow between chained operators."""

    def test_tensor_passthrough_chain(self, lf, lfs_types, numpy):
        """Tensor should pass through multiple operators unchanged."""

        class PassthroughOp(lfs_types.Operator):
            lf_label = "Passthrough"

            def execute(self, context):
                t = getattr(self, "data", None)
                return {"status": "FINISHED", "data": t}

        lf.register_class(PassthroughOp)
        try:
            original = lf.Tensor.arange(0, 100, dtype="float32", device="cpu")
            original_sum = original.sum().item()

            # Chain through multiple operators
            r1 = lf.ops.invoke(PassthroughOp._class_id(), data=original)
            r2 = lf.ops.invoke(PassthroughOp._class_id(), data=r1.data)
            r3 = lf.ops.invoke(PassthroughOp._class_id(), data=r2.data)

            # Data should be preserved
            assert r3.data.sum().item() == original_sum
            assert tuple(r3.data.shape) == tuple(original.shape)
        finally:
            lf.unregister_class(PassthroughOp)

    def test_processing_pipeline(self, lf, lfs_types, numpy):
        """Test a realistic processing pipeline."""

        class ScaleOp(lfs_types.Operator):
            lf_label = "Scale"

            def execute(self, context):
                t = getattr(self, "tensor", None)
                factor = getattr(self, "factor", 1.0)
                if t is None:
                    return {"CANCELLED"}
                result = t * factor
                return {"status": "FINISHED", "tensor": result}

        class OffsetOp(lfs_types.Operator):
            lf_label = "Offset"

            def execute(self, context):
                t = getattr(self, "tensor", None)
                offset = getattr(self, "offset", 0.0)
                if t is None:
                    return {"CANCELLED"}
                result = t + offset
                return {"status": "FINISHED", "tensor": result}

        class ClampOp(lfs_types.Operator):
            lf_label = "Clamp"

            def execute(self, context):
                t = getattr(self, "tensor", None)
                min_val = getattr(self, "min_val", 0.0)
                max_val = getattr(self, "max_val", 1.0)
                if t is None:
                    return {"CANCELLED"}
                result = t.clamp(min_val, max_val)
                return {"status": "FINISHED", "tensor": result}

        lf.register_class(ScaleOp)
        lf.register_class(OffsetOp)
        lf.register_class(ClampOp)
        try:
            # Create input: [0, 1, 2, 3, 4]
            input_t = lf.Tensor.arange(0, 5, dtype="float32", device="cpu")

            # Pipeline: scale by 2, offset by -3, clamp to [0, 5]
            # [0, 1, 2, 3, 4] * 2 = [0, 2, 4, 6, 8]
            # [0, 2, 4, 6, 8] - 3 = [-3, -1, 1, 3, 5]
            # clamp to [0, 5] = [0, 0, 1, 3, 5]

            r1 = lf.ops.invoke(ScaleOp._class_id(), tensor=input_t, factor=2.0)
            assert r1.finished

            r2 = lf.ops.invoke(OffsetOp._class_id(), tensor=r1.tensor, offset=-3.0)
            assert r2.finished

            r3 = lf.ops.invoke(ClampOp._class_id(), tensor=r2.tensor, min_val=0.0, max_val=5.0)
            assert r3.finished

            result = r3.tensor.numpy()
            expected = [0, 0, 1, 3, 5]
            for i, (got, exp) in enumerate(zip(result, expected)):
                assert abs(got - exp) < 0.001, f"Mismatch at {i}: {got} != {exp}"
        finally:
            lf.unregister_class(ScaleOp)
            lf.unregister_class(OffsetOp)
            lf.unregister_class(ClampOp)


# =============================================================================
# PropertyGroup Tests
# =============================================================================


class TestPropertyGroupBasics:
    """Test PropertyGroup fundamentals."""

    def test_property_default_values(self, lfs_props, lfs_types):
        """Properties should initialize to their defaults."""

        class DefaultsGroup(lfs_types.PropertyGroup):
            int_prop = lfs_props.IntProperty(default=42)
            float_prop = lfs_props.FloatProperty(default=3.14)
            bool_prop = lfs_props.BoolProperty(default=True)
            string_prop = lfs_props.StringProperty(default="hello")

        g = DefaultsGroup()
        assert g.int_prop == 42
        assert abs(g.float_prop - 3.14) < 0.001
        assert g.bool_prop is True
        assert g.string_prop == "hello"

    def test_property_assignment(self, lfs_props, lfs_types):
        """Properties should accept new values."""

        class AssignGroup(lfs_types.PropertyGroup):
            value = lfs_props.IntProperty(default=0)

        g = AssignGroup()
        assert g.value == 0

        g.value = 100
        assert g.value == 100

        g.value = -50
        assert g.value == -50

    def test_property_validation_clamps_values(self, lfs_props, lfs_types):
        """Properties with min/max should clamp values."""

        class ClampGroup(lfs_types.PropertyGroup):
            clamped = lfs_props.IntProperty(default=50, min=0, max=100)

        g = ClampGroup()

        g.clamped = 150  # Over max
        assert g.clamped == 100

        g.clamped = -50  # Under min
        assert g.clamped == 0

        g.clamped = 75  # In range
        assert g.clamped == 75

    def test_float_property_precision(self, lfs_props, lfs_types):
        """Float properties should handle precision correctly."""

        class PrecisionGroup(lfs_types.PropertyGroup):
            precise = lfs_props.FloatProperty(default=0.0, precision=6)

        g = PrecisionGroup()

        g.precise = 0.123456789
        assert abs(g.precise - 0.123456789) < 0.0000001

    def test_string_property_maxlen(self, lfs_props, lfs_types):
        """String properties should truncate to maxlen."""

        class MaxlenGroup(lfs_types.PropertyGroup):
            short = lfs_props.StringProperty(default="", maxlen=5)

        g = MaxlenGroup()

        g.short = "hello world"
        assert g.short == "hello"
        assert len(g.short) == 5


class TestPropertyGroupVectors:
    """Test vector property types."""

    def test_float_vector_property(self, lfs_props, lfs_types):
        """FloatVectorProperty should handle tuples correctly."""

        class VectorGroup(lfs_types.PropertyGroup):
            position = lfs_props.FloatVectorProperty(default=(0, 0, 0), size=3)
            color = lfs_props.FloatVectorProperty(default=(1, 1, 1, 1), size=4)

        g = VectorGroup()

        assert g.position == (0.0, 0.0, 0.0)
        assert g.color == (1.0, 1.0, 1.0, 1.0)

        g.position = (1.5, 2.5, 3.5)
        assert g.position == (1.5, 2.5, 3.5)

    def test_int_vector_property(self, lfs_props, lfs_types):
        """IntVectorProperty should handle integer tuples."""

        class IntVecGroup(lfs_types.PropertyGroup):
            dimensions = lfs_props.IntVectorProperty(default=(640, 480), size=2)

        g = IntVecGroup()

        assert g.dimensions == (640, 480)

        g.dimensions = (1920, 1080)
        assert g.dimensions == (1920, 1080)

    def test_vector_from_list(self, lfs_props, lfs_types):
        """Vector properties should accept lists as well as tuples."""

        class ListVecGroup(lfs_types.PropertyGroup):
            vec = lfs_props.FloatVectorProperty(default=(0, 0, 0), size=3)

        g = ListVecGroup()

        g.vec = [1.0, 2.0, 3.0]  # List instead of tuple
        assert g.vec == (1.0, 2.0, 3.0)

    def test_vector_clamping(self, lfs_props, lfs_types):
        """Vector elements should be clamped to min/max."""

        class ClampVecGroup(lfs_types.PropertyGroup):
            color = lfs_props.FloatVectorProperty(
                default=(0.5, 0.5, 0.5), size=3, min=0.0, max=1.0
            )

        g = ClampVecGroup()

        g.color = (1.5, -0.5, 0.5)  # Out of range values
        assert g.color == (1.0, 0.0, 0.5)


class TestPropertyGroupEnums:
    """Test enum property types."""

    def test_enum_property_basic(self, lfs_props, lfs_types):
        """EnumProperty should accept valid enum values."""

        class EnumGroup(lfs_types.PropertyGroup):
            mode = lfs_props.EnumProperty(
                items=[
                    ("ADD", "Add", "Add mode"),
                    ("SUB", "Subtract", "Subtract mode"),
                    ("MUL", "Multiply", "Multiply mode"),
                ],
                default="ADD",
            )

        g = EnumGroup()

        assert g.mode == "ADD"

        g.mode = "SUB"
        assert g.mode == "SUB"

        g.mode = "MUL"
        assert g.mode == "MUL"

    def test_enum_invalid_value_uses_default(self, lfs_props, lfs_types):
        """Invalid enum values should fall back to default."""

        class EnumFallbackGroup(lfs_types.PropertyGroup):
            choice = lfs_props.EnumProperty(
                items=[("A", "Option A", ""), ("B", "Option B", "")],
                default="A",
            )

        g = EnumFallbackGroup()

        g.choice = "INVALID"
        assert g.choice == "A"  # Falls back to default


class TestPropertyGroupTensors:
    """Test TensorProperty in PropertyGroups."""

    def test_tensor_property_none_default(self, lf, lfs_props, lfs_types):
        """TensorProperty should default to None."""

        class TensorGroup(lfs_types.PropertyGroup):
            data = lfs_props.TensorProperty(shape=(-1, 3), dtype="float32", device="cpu")

        g = TensorGroup()
        assert g.data is None

    def test_tensor_property_set_and_get(self, lf, lfs_props, lfs_types):
        """TensorProperty should store and retrieve tensors."""

        class TensorGroup(lfs_types.PropertyGroup):
            points = lfs_props.TensorProperty(shape=(-1, 3), dtype="float32", device="cpu")

        g = TensorGroup()

        t = lf.Tensor.zeros([100, 3], dtype="float32", device="cpu")
        g.points = t

        assert g.points is not None
        assert tuple(g.points.shape) == (100, 3)

    def test_tensor_property_validation_rejects_bad_shape(self, lf, lfs_props, lfs_types):
        """TensorProperty should reject tensors with wrong shape."""

        class StrictShapeGroup(lfs_types.PropertyGroup):
            matrix = lfs_props.TensorProperty(shape=(4, 4), dtype="float32", device="cpu")

        g = StrictShapeGroup()

        with pytest.raises(ValueError, match="Shape mismatch"):
            g.matrix = lf.Tensor.zeros([3, 3], dtype="float32", device="cpu")

    def test_tensor_property_validation_rejects_bad_dtype(self, lf, lfs_props, lfs_types):
        """TensorProperty should reject tensors with wrong dtype."""

        class StrictDtypeGroup(lfs_types.PropertyGroup):
            ints = lfs_props.TensorProperty(shape=(-1,), dtype="int32", device="cpu")

        g = StrictDtypeGroup()

        with pytest.raises(ValueError, match="dtype"):
            g.ints = lf.Tensor.zeros([10], dtype="float32", device="cpu")

    def test_tensor_property_clear_with_none(self, lf, lfs_props, lfs_types):
        """Setting TensorProperty to None should clear it."""

        class ClearableGroup(lfs_types.PropertyGroup):
            data = lfs_props.TensorProperty(shape=(-1,), dtype="float32", device="cpu")

        g = ClearableGroup()

        g.data = lf.Tensor.ones([50], dtype="float32", device="cpu")
        assert g.data is not None

        g.data = None
        assert g.data is None

    def test_multiple_tensor_properties(self, lf, lfs_props, lfs_types):
        """PropertyGroup should handle multiple TensorProperties."""

        class MultiTensorGroup(lfs_types.PropertyGroup):
            positions = lfs_props.TensorProperty(shape=(-1, 3), dtype="float32", device="cpu")
            normals = lfs_props.TensorProperty(shape=(-1, 3), dtype="float32", device="cpu")
            colors = lfs_props.TensorProperty(shape=(-1, 4), dtype="float32", device="cpu")
            indices = lfs_props.TensorProperty(shape=(-1,), dtype="int32", device="cpu")

        g = MultiTensorGroup()

        g.positions = lf.Tensor.zeros([100, 3], dtype="float32", device="cpu")
        g.normals = lf.Tensor.zeros([100, 3], dtype="float32", device="cpu")
        g.colors = lf.Tensor.ones([100, 4], dtype="float32", device="cpu")
        g.indices = lf.Tensor.arange(0, 100, dtype="int32", device="cpu")

        assert tuple(g.positions.shape) == (100, 3)
        assert tuple(g.normals.shape) == (100, 3)
        assert tuple(g.colors.shape) == (100, 4)
        assert tuple(g.indices.shape) == (100,)


class TestPropertyGroupCallbacks:
    """Test property update callbacks."""

    def test_update_callback_fires_on_change(self, lfs_props, lfs_types):
        """Update callback should fire when property changes."""
        callback_fired = [0]

        def on_update(self, context):
            callback_fired[0] += 1

        class CallbackGroup(lfs_types.PropertyGroup):
            value = lfs_props.IntProperty(default=0, update=on_update)

        g = CallbackGroup()

        g.value = 10
        assert callback_fired[0] == 1

        g.value = 20
        assert callback_fired[0] == 2

    def test_update_callback_receives_self(self, lfs_props, lfs_types):
        """Update callback should receive the PropertyGroup instance."""
        received_self = [None]

        def on_update(self, context):
            received_self[0] = self

        class SelfCallbackGroup(lfs_types.PropertyGroup):
            value = lfs_props.IntProperty(default=0, update=on_update)

        g = SelfCallbackGroup()
        g.value = 42

        assert received_self[0] is g

    def test_callback_exception_doesnt_crash(self, lfs_props, lfs_types):
        """Exception in callback should not crash."""

        def bad_callback(self, context):
            raise RuntimeError("Callback error")

        class BadCallbackGroup(lfs_types.PropertyGroup):
            value = lfs_props.IntProperty(default=0, update=bad_callback)

        g = BadCallbackGroup()

        # Should not raise
        g.value = 10
        assert g.value == 10


class TestPropertyGroupMixedTypes:
    """Test PropertyGroups with mixed property types."""

    def test_mixed_property_types(self, lf, lfs_props, lfs_types):
        """PropertyGroup should handle all property types together."""

        class MixedGroup(lfs_types.PropertyGroup):
            name = lfs_props.StringProperty(default="untitled")
            enabled = lfs_props.BoolProperty(default=True)
            opacity = lfs_props.FloatProperty(default=1.0, min=0.0, max=1.0)
            count = lfs_props.IntProperty(default=0)
            color = lfs_props.FloatVectorProperty(default=(1, 1, 1), size=3)
            mode = lfs_props.EnumProperty(
                items=[("A", "A", ""), ("B", "B", "")], default="A"
            )
            data = lfs_props.TensorProperty(shape=(-1,), dtype="float32", device="cpu")

        g = MixedGroup()

        # Set all properties
        g.name = "test_object"
        g.enabled = False
        g.opacity = 0.5
        g.count = 42
        g.color = (0.5, 0.5, 0.5)
        g.mode = "B"
        g.data = lf.Tensor.ones([10], dtype="float32", device="cpu")

        # Verify all
        assert g.name == "test_object"
        assert g.enabled is False
        assert abs(g.opacity - 0.5) < 0.001
        assert g.count == 42
        assert g.color == (0.5, 0.5, 0.5)
        assert g.mode == "B"
        assert g.data.sum().item() == 10.0


class TestPropertyGroupInheritance:
    """Test PropertyGroup inheritance."""

    def test_inherited_properties(self, lfs_props, lfs_types):
        """Subclass should inherit parent properties."""

        class BaseGroup(lfs_types.PropertyGroup):
            base_prop = lfs_props.IntProperty(default=10)

        class DerivedGroup(BaseGroup):
            derived_prop = lfs_props.IntProperty(default=20)

        g = DerivedGroup()

        assert g.base_prop == 10
        assert g.derived_prop == 20

    def test_property_override(self, lfs_props, lfs_types):
        """Subclass can override parent property defaults."""

        class BaseGroup(lfs_types.PropertyGroup):
            value = lfs_props.IntProperty(default=100)

        class OverrideGroup(BaseGroup):
            value = lfs_props.IntProperty(default=200)

        base = BaseGroup()
        derived = OverrideGroup()

        assert base.value == 100
        assert derived.value == 200


# =============================================================================
# Operator + PropertyGroup Integration Tests
# =============================================================================


class TestOperatorPropertyGroupIntegration:
    """Test operators that use PropertyGroups for settings."""

    def test_operator_reads_property_group(self, lf, lfs_props, lfs_types):
        """Operator should be able to read from PropertyGroup."""

        class FilterSettings(lfs_types.PropertyGroup):
            threshold = lfs_props.FloatProperty(default=0.5, min=0.0, max=1.0)
            invert = lfs_props.BoolProperty(default=False)

        class FilterOp(lfs_types.Operator):
            lf_label = "Filter"

            def execute(self, context):
                settings = getattr(self, "settings", None)
                if settings is None:
                    return {"CANCELLED"}
                return {
                    "status": "FINISHED",
                    "threshold": settings.threshold,
                    "invert": settings.invert,
                }

        lf.register_class(FilterOp)
        try:
            settings = FilterSettings()
            settings.threshold = 0.75
            settings.invert = True

            result = lf.ops.invoke(FilterOp._class_id(), settings=settings)

            assert result.finished
            assert result.threshold == 0.75
            assert result.invert is True
        finally:
            lf.unregister_class(FilterOp)

    def test_operator_modifies_property_group(self, lf, lfs_props, lfs_types):
        """Operator should be able to modify PropertyGroup values."""

        class CounterSettings(lfs_types.PropertyGroup):
            count = lfs_props.IntProperty(default=0)

        class IncrementOp(lfs_types.Operator):
            lf_label = "Increment"

            def execute(self, context):
                settings = getattr(self, "settings", None)
                if settings is None:
                    return {"CANCELLED"}
                settings.count += 1
                return {"status": "FINISHED", "new_count": settings.count}

        lf.register_class(IncrementOp)
        try:
            settings = CounterSettings()
            assert settings.count == 0

            r1 = lf.ops.invoke(IncrementOp._class_id(), settings=settings)
            assert r1.new_count == 1
            assert settings.count == 1

            r2 = lf.ops.invoke(IncrementOp._class_id(), settings=settings)
            assert r2.new_count == 2
            assert settings.count == 2
        finally:
            lf.unregister_class(IncrementOp)

    def test_operator_with_tensor_in_property_group(self, lf, lfs_props, lfs_types, numpy):
        """Operator should work with TensorProperty in PropertyGroup."""

        class DataSettings(lfs_types.PropertyGroup):
            points = lfs_props.TensorProperty(shape=(-1, 3), dtype="float32", device="cpu")

        class ProcessDataOp(lfs_types.Operator):
            lf_label = "Process Data"

            def execute(self, context):
                settings = getattr(self, "settings", None)
                if settings is None or settings.points is None:
                    return {"CANCELLED"}

                # Compute centroid
                centroid = settings.points.mean(dim=0)
                return {"status": "FINISHED", "centroid": centroid}

        lf.register_class(ProcessDataOp)
        try:
            settings = DataSettings()

            # Create point cloud around (10, 20, 30)
            points = lf.Tensor.zeros([100, 3], dtype="float32", device="cpu")
            points[:, 0] = 10.0
            points[:, 1] = 20.0
            points[:, 2] = 30.0
            settings.points = points

            result = lf.ops.invoke(ProcessDataOp._class_id(), settings=settings)

            assert result.finished
            centroid = result.centroid.numpy()
            assert abs(centroid[0] - 10.0) < 0.001
            assert abs(centroid[1] - 20.0) < 0.001
            assert abs(centroid[2] - 30.0) < 0.001
        finally:
            lf.unregister_class(ProcessDataOp)


# =============================================================================
# Edge Case Tests
# =============================================================================


class TestEdgeCases:
    """Test edge cases and boundary conditions."""

    def test_empty_tensor_handling(self, lf, lfs_types):
        """Operators should handle empty tensors gracefully."""

        class EmptyTensorOp(lfs_types.Operator):
            lf_label = "Empty Tensor"

            def execute(self, context):
                t = getattr(self, "tensor", None)
                if t is None:
                    return {"CANCELLED"}
                return {
                    "status": "FINISHED",
                    "numel": t.numel,
                    "shape": tuple(t.shape),
                }

        lf.register_class(EmptyTensorOp)
        try:
            empty = lf.Tensor.zeros([0, 3], dtype="float32", device="cpu")
            result = lf.ops.invoke(EmptyTensorOp._class_id(), tensor=empty)

            assert result.finished
            assert result.numel == 0
            assert result.shape == (0, 3)
        finally:
            lf.unregister_class(EmptyTensorOp)

    def test_large_tensor_handling(self, lf, lfs_types, numpy):
        """Operators should handle large tensors."""

        class LargeTensorOp(lfs_types.Operator):
            lf_label = "Large Tensor"

            def execute(self, context):
                t = getattr(self, "tensor", None)
                if t is None:
                    return {"CANCELLED"}
                return {"status": "FINISHED", "sum": t.sum().item()}

        lf.register_class(LargeTensorOp)
        try:
            # 1 million points
            large = lf.Tensor.ones([1000000, 3], dtype="float32", device="cpu")
            result = lf.ops.invoke(LargeTensorOp._class_id(), tensor=large)

            assert result.finished
            assert result.sum == 3000000.0
        finally:
            lf.unregister_class(LargeTensorOp)

    def test_unicode_in_properties(self, lfs_props, lfs_types):
        """Properties should handle unicode strings."""

        class UnicodeGroup(lfs_types.PropertyGroup):
            name = lfs_props.StringProperty(default="")

        g = UnicodeGroup()

        g.name = "æ—¥æœ¬èªžãƒ†ã‚¹ãƒˆ"
        assert g.name == "æ—¥æœ¬èªžãƒ†ã‚¹ãƒˆ"

        g.name = "Ã©mojis: ðŸŽ¨ðŸ”¥âœ¨"
        assert g.name == "Ã©mojis: ðŸŽ¨ðŸ”¥âœ¨"

    def test_special_float_values(self, lfs_props, lfs_types):
        """Properties should handle special float values."""
        import math

        class SpecialFloatGroup(lfs_types.PropertyGroup):
            value = lfs_props.FloatProperty(default=0.0)

        g = SpecialFloatGroup()

        # Very small values
        g.value = 1e-30
        assert g.value == 1e-30

        # Very large values
        g.value = 1e30
        assert g.value == 1e30

    def test_rapid_property_updates(self, lfs_props, lfs_types):
        """Properties should handle rapid updates."""
        update_count = [0]

        def on_update(self, context):
            update_count[0] += 1

        class RapidUpdateGroup(lfs_types.PropertyGroup):
            value = lfs_props.IntProperty(default=0, update=on_update)

        g = RapidUpdateGroup()

        # Simulate rapid slider movement
        for i in range(1000):
            g.value = i

        assert g.value == 999
        assert update_count[0] == 1000

    def test_property_group_get_all_properties(self, lf, lfs_props, lfs_types):
        """get_all_properties should return all defined properties."""

        class AllPropsGroup(lfs_types.PropertyGroup):
            a = lfs_props.IntProperty(default=1)
            b = lfs_props.FloatProperty(default=2.0)
            c = lfs_props.StringProperty(default="three")
            d = lfs_props.TensorProperty(shape=(-1,), dtype="float32", device="cpu")

        g = AllPropsGroup()
        props = g.get_all_properties()

        assert "a" in props
        assert "b" in props
        assert "c" in props
        assert "d" in props
        assert len(props) == 4


class TestConcurrentAccess:
    """Test thread-safety concerns (single-threaded simulation)."""

    def test_multiple_property_group_instances(self, lfs_props, lfs_types):
        """Multiple PropertyGroup instances should be independent."""

        class IndependentGroup(lfs_types.PropertyGroup):
            value = lfs_props.IntProperty(default=0)

        g1 = IndependentGroup()
        g2 = IndependentGroup()
        g3 = IndependentGroup()

        g1.value = 100
        g2.value = 200
        g3.value = 300

        assert g1.value == 100
        assert g2.value == 200
        assert g3.value == 300

    def test_operator_instances_isolated(self, lf, lfs_types):
        """Multiple operator invocations should be isolated."""
        invocation_data = []

        class RecordOp(lfs_types.Operator):
            lf_label = "Record"

            def execute(self, context):
                invocation_data.append(
                    {
                        "instance_id": id(self),
                        "value": getattr(self, "value", None),
                    }
                )
                return {"FINISHED"}

        lf.register_class(RecordOp)
        try:
            lf.ops.invoke(RecordOp._class_id(), value="first")
            lf.ops.invoke(RecordOp._class_id(), value="second")
            lf.ops.invoke(RecordOp._class_id(), value="third")

            assert len(invocation_data) == 3
            assert invocation_data[0]["value"] == "first"
            assert invocation_data[1]["value"] == "second"
            assert invocation_data[2]["value"] == "third"
        finally:
            lf.unregister_class(RecordOp)


# =============================================================================
# GPU-Specific Tests
# =============================================================================


class TestGPUOperations:
    """Test GPU tensor operations in operators."""

    def test_cuda_tensor_in_operator(self, lf, lfs_types, numpy):
        """Operator should handle CUDA tensors."""

        class CudaOp(lfs_types.Operator):
            lf_label = "CUDA Op"

            def execute(self, context):
                t = getattr(self, "tensor", None)
                if t is None:
                    return {"CANCELLED"}
                return {
                    "status": "FINISHED",
                    "device": str(t.device),
                    "sum": t.sum().item(),
                }

        lf.register_class(CudaOp)
        try:
            cuda_tensor = lf.Tensor.ones([1000], dtype="float32", device="cuda")
            result = lf.ops.invoke(CudaOp._class_id(), tensor=cuda_tensor)

            assert result.finished
            assert result.device == "cuda"
            assert result.sum == 1000.0
        finally:
            lf.unregister_class(CudaOp)

    def test_cuda_to_cpu_in_operator(self, lf, lfs_types, numpy):
        """Operator should handle CUDA to CPU transfer."""

        class TransferOp(lfs_types.Operator):
            lf_label = "Transfer Op"

            def execute(self, context):
                t = getattr(self, "tensor", None)
                if t is None:
                    return {"CANCELLED"}
                cpu_t = t.cpu()
                return {
                    "status": "FINISHED",
                    "cpu_tensor": cpu_t,
                    "original_device": str(t.device),
                }

        lf.register_class(TransferOp)
        try:
            cuda_tensor = lf.Tensor.arange(0, 10, dtype="float32", device="cuda")
            result = lf.ops.invoke(TransferOp._class_id(), tensor=cuda_tensor)

            assert result.finished
            assert result.original_device == "cuda"
            assert str(result.cpu_tensor.device) == "cpu"
        finally:
            lf.unregister_class(TransferOp)

    def test_tensor_property_cuda_validation(self, lf, lfs_props, lfs_types):
        """TensorProperty should enforce CUDA device."""

        class CudaOnlyGroup(lfs_types.PropertyGroup):
            gpu_data = lfs_props.TensorProperty(
                shape=(-1, 3), dtype="float32", device="cuda"
            )

        g = CudaOnlyGroup()

        # CPU tensor should be rejected
        cpu_t = lf.Tensor.zeros([100, 3], dtype="float32", device="cpu")
        with pytest.raises(ValueError, match="device"):
            g.gpu_data = cpu_t

        # CUDA tensor should be accepted
        cuda_t = lf.Tensor.zeros([100, 3], dtype="float32", device="cuda")
        g.gpu_data = cuda_t
        assert str(g.gpu_data.device) == "cuda"
